{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOOrNZ7Y2SzGjx+ItphwIJW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palindromeRice/Moe_demo/blob/main/Mixture_of_Experts_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixture of Experts: Simplified Prompt Routing\n",
        "\n",
        "Here I tried to explain the core idea behind **Mixture of Experts (MoE)** using a minimal, embedding-based routing system.\n",
        "\n",
        "We define expert models and route user prompts to the most relevant one based on semantic similarity.\n"
      ],
      "metadata": {
        "id": "FlvsX0vVSxAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we install all the libraries we'll need:\n",
        "- `transformers`: for using pre-trained language models.\n",
        "- `sentence-transformers`: for turning text into useful number-based representations (called embeddings).\n",
        "\n"
      ],
      "metadata": {
        "id": "oUeidBDzSGOM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRwdJVguP6LR",
        "outputId": "c846352d-28e7-437d-bbdf-a0cabadb6bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q transformers sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the libraries we just installed and set up our embedding model.\n",
        "\n",
        "This model will help us understand the meaning of both the user's prompt and the expert descriptions by converting them into vectors (think of them as smart number lists).\n"
      ],
      "metadata": {
        "id": "W3RbdliHSL-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "mbBfFnlnUA_q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "BLUSPsTiR21Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define a list of \"experts.\" Each expert has:\n",
        "- A short description of what they do\n",
        "- The model they'll use\n",
        "- The kind of task they handle (like text generation or classification)\n",
        "\n",
        "We also prepare a cache to store expert embeddings so we don't have to recompute them every time.\n"
      ],
      "metadata": {
        "id": "Qn6MmUdWSO_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expert_profiles = {\n",
        "    \"codet5-small\": {\n",
        "        \"description\": \"Code generation expert: generates syntactically correct Python functions and code snippets from natural language instructions.\",\n",
        "        \"model_name\": \"Salesforce/codet5-small\",\n",
        "        \"task\": \"text2text-generation\"\n",
        "    },\n",
        "    \"t5-commongen\": {\n",
        "        \"description\": \"Text transformation expert: rephrases or refines general text without specializing in code.\",\n",
        "        \"model_name\": \"mrm8488/t5-small-finetuned-common_gen\",\n",
        "        \"task\": \"text2text-generation\"\n",
        "    },\n",
        "    \"qa\": {\n",
        "        \"description\": \"Question answering expert: extracts answers from context given a question.\",\n",
        "        \"model_name\": \"distilbert-base-uncased-distilled-squad\",\n",
        "        \"task\": \"question-answering\"\n",
        "    },\n",
        "    \"classification\": {\n",
        "        \"description\": \"Text classification expert: analyzes sentiment and categorizes text.\",\n",
        "        \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "        \"task\": \"sentiment-analysis\"\n",
        "    },\n",
        "    \"tiny-gpt2\": {\n",
        "        \"description\": \"Open-ended text generation expert: generates creative text not specific to code.\",\n",
        "        \"model_name\": \"sshleifer/tiny-gpt2\",\n",
        "        \"task\": \"text-generation\"\n",
        "    }\n",
        "}\n",
        "\n",
        "_expert_embedding_cache = {}\n"
      ],
      "metadata": {
        "id": "lU_QLuGHR6fw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define two functions:\n",
        "1. One to get and store the expert descriptions as embeddings.\n",
        "2. Another to match a user prompt with the most relevant expert(s), based on how similar the meanings are.\n",
        "\n",
        "We'll use this to figure out which expert is best for answering a prompt.\n"
      ],
      "metadata": {
        "id": "ZV5r56YGSQ3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cached_expert_embeddings(profiles):\n",
        "    global _expert_embedding_cache\n",
        "    if not _expert_embedding_cache:\n",
        "        expert_names = list(profiles.keys())\n",
        "        expert_descriptions = [profiles[expert][\"description\"] for expert in expert_names]\n",
        "        embeddings = embedder.encode(expert_descriptions, convert_to_tensor=True)\n",
        "        _expert_embedding_cache = {name: emb for name, emb in zip(expert_names, embeddings)}\n",
        "        print(\"Expert embeddings computed and cached.\")\n",
        "    return _expert_embedding_cache\n",
        "\n",
        "def route_prompt_to_experts(prompt, profiles, threshold=0.3, top_k=3):\n",
        "    expert_embeddings = get_cached_expert_embeddings(profiles)\n",
        "    expert_names = list(expert_embeddings.keys())\n",
        "    embeddings_tensor = torch.stack([expert_embeddings[name] for name in expert_names])\n",
        "    prompt_embedding = embedder.encode(prompt, convert_to_tensor=True)\n",
        "    cosine_scores = util.cos_sim(prompt_embedding, embeddings_tensor)[0]\n",
        "    expert_scores = {name: score.item() for name, score in zip(expert_names, cosine_scores)}\n",
        "    for name, score in expert_scores.items():\n",
        "        print(f\"Expert '{name}' similarity: {score:.4f}\")\n",
        "    filtered_experts = {name: score for name, score in expert_scores.items() if score >= threshold}\n",
        "    if not filtered_experts:\n",
        "        print(\"No expert met the threshold. Using all expert scores for selection.\")\n",
        "        filtered_experts = expert_scores\n",
        "    sorted_experts = sorted(filtered_experts.items(), key=lambda item: item[1], reverse=True)\n",
        "    selected_experts = [name for name, score in sorted_experts[:top_k]]\n",
        "    print(f\"Selected experts: {selected_experts}\")\n",
        "    return selected_experts\n"
      ],
      "metadata": {
        "id": "RLzq-SutR8MU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this final step, we test everything using an example prompt\n",
        "\n",
        "We’ll see how similar this prompt is to each expert's description and choose the most relevant one based on that.\n"
      ],
      "metadata": {
        "id": "3DN8Fq5cSSqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_prompt = \"Write a Python function to reverse a string.\"\n",
        "selected_experts = route_prompt_to_experts(sample_prompt, expert_profiles, threshold=0.3, top_k=1)\n",
        "print(\"Final selected expert(s):\", selected_experts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp9GgAyWR-B3",
        "outputId": "c9ac83ab-1291-4d55-8c0d-516c0c495c2e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expert embeddings computed and cached.\n",
            "Expert 'codet5-small' similarity: 0.3539\n",
            "Expert 't5-commongen' similarity: 0.2246\n",
            "Expert 'qa' similarity: 0.1086\n",
            "Expert 'classification' similarity: 0.0846\n",
            "Expert 'tiny-gpt2' similarity: 0.1466\n",
            "Selected experts: ['codet5-small']\n",
            "Final selected expert(s): ['codet5-small']\n"
          ]
        }
      ]
    }
  ]
}